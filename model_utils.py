# -*- coding: utf-8 -*-
"""deploy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HZzmlHMtWRlVRadRh1bnhoiZlLo-hGgD
"""

import pandas as pd
import torch
import random
import numpy as np
import streamlit as st

import re
def remove_emojis(data):
    emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
                      "]+", re.UNICODE)
    return re.sub(emoj, ' ', data)

def preprocess_tokenize(text):
      # for removing punctuation from sentencesc
    text = str(text)
    #text = re.sub(r'(\d+)', r'', text)

    text = text.replace('\n', ' ')
    text = text.replace('\r', ' ')
    text = text.replace('\t', ' ')
    text = text.replace('\u200d', '')
    text=re.sub("(__+)", ' ', str(text)).lower()   #remove _ if it occors more than one time consecutively
    text=re.sub("(--+)", ' ', str(text)).lower()   #remove - if it occors more than one time consecutively
    text=re.sub("(~~+)", ' ', str(text)).lower()   #remove ~ if it occors more than one time consecutively
    text=re.sub("(\+\++)", ' ', str(text)).lower()   #remove + if it occors more than one time consecutively
    text=re.sub("(\.\.+)", ' ', str(text)).lower()   #remove . if it occors more than one time consecutively
    text=re.sub(r"[<>()|&©@#ø\[\]\'\",;:?.~*!]", ' ', str(text)).lower() #remove <>()|&©ø"',;?~*!
    text = re.sub(r"[‘’।:]", " ", str(text)) #removing other special characters
    text = re.sub("([a-zA-Z])",' ',str(text)).lower()
    text = re.sub("(\s+)",' ',str(text)).lower()
    text = remove_emojis(text)
    return text

# Define a function for inference on a single news article
def generate_summary(news_text, model, tokenizer, max_length=1000, num_beams=4, length_penalty=2.0):
    # Tokenize the input news text
    inputs = tokenizer.encode("summarize: " + news_text, return_tensors="pt", max_length=max_length, truncation=True)

    # Generate the summary
    summary_ids = model.generate(inputs.to(model.device), max_length=max_length, num_beams=num_beams, length_penalty=length_penalty, early_stopping=True)

    # Decode the summary IDs to text
    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    return summary_text

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load the saved model
model1 = AutoModelForSeq2SeqLM.from_pretrained("C:/Users/gaura/OneDrive/Desktop/hindi news/deploy/archive (3)/mbart_model.pt")
# Load the tokenizer
tokenizer1 = AutoTokenizer.from_pretrained("C:/Users/gaura/OneDrive/Desktop/hindi news/deploy/archive (3)/mbart_tokenizer.json")

# Load the saved model
model2 = AutoModelForSeq2SeqLM.from_pretrained("C:/Users/gaura/OneDrive/Desktop/hindi news/deploy/archive (2)/mt5_model.pt")
# Load the tokenizer
tokenizer2 = AutoTokenizer.from_pretrained("C:/Users/gaura/OneDrive/Desktop/hindi news/deploy/archive (2)/mt5_tokenizer.json")

# Move the model to the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# Move the model to the device (CPU or GPU)
model1.to(device)

model2.to(device)



